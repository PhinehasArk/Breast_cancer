# -*- coding: utf-8 -*-
"""Clustering_UMAP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Z9eRcr7BCBOpYbxTXabcWXHjvFpC1tln
"""

!pip install umap-learn 
!pip install dataprep # data exploration
!pip install shap # model interpretation

# Commented out IPython magic to ensure Python compatibility.
# Import of Packages


# Packages to load and preprocess data
import numpy as np
import pandas as pd
import io

# Packages to visualise and explore data
import seaborn as sns
sns.set_style("whitegrid")
import matplotlib.pyplot as plt
# from dataprep.eda import plot, create_report
from dataprep.eda import plot, create_report, plot_missing, plot_correlation

# Packages to prepare data for ML  
from sklearn.preprocessing import OrdinalEncoder
from sklearn.model_selection import GridSearchCV, KFold
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import roc_curve
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
# %matplotlib inline

# Package to interpret data
import shap


# Ignore the warnings notifications
import warnings
warnings.filterwarnings("ignore")

# Connect to google drive 
from google.colab import drive
drive.mount('/content/drive')

df = pd.read_excel(('/content/drive/MyDrive/Project/finaldata.xlsx'))
# Dataset is now stored in a Pandas Dataframe

# Have a quick look on data
df.info()

# Data information
df.head()

#  Check duplicated values
print('The number of duplicated values in data:',
         df.duplicated().sum())

#  Drop unused cols: Based on df.info(), we will drop some unused cols and null cols

drop_list = ['cell_id',"cell_line"]
df = df.drop(drop_list, axis=1)

# We check the number of cell lines by cancer type
print('\nGroup cell lines',df.groupby('BCtype')['cluster'].count())

# check the shape of data after column drop 
df

"""# New Section"""

print(df.shape)

df.info()

report = create_report(df, title='BC classification by subtypes')

report

report.save('/content/drive/MyDrive/Project/EDA_BCtype_prediction.html')

## Scaling of Data

y = df["BCtype"]

y

from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
X =  pd.DataFrame(sc_X.fit_transform(df.drop(["BCtype",],axis = 1),),
        columns=['UMAMP_X', 'UMAP_Y',  'cluster', ])

X.head()

#importing train_test_split
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=1/3,random_state=42, stratify=y)

from sklearn.neighbors import KNeighborsClassifier


test_scores = []
train_scores = []

for i in range(1,15):

    knn = KNeighborsClassifier(i)
    knn.fit(X_train,y_train)
    
    train_scores.append(knn.score(X_train,y_train))
    test_scores.append(knn.score(X_test,y_test))

## score that comes from testing on the same datapoints that were used for training
max_train_score = max(train_scores)
train_scores_ind = [i for i, v in enumerate(train_scores) if v == max_train_score]
print('Max train score {} % and k = {}'.format(max_train_score*100,list(map(lambda x: x+1, train_scores_ind))))

## score that comes from testing on the datapoints that were split in the beginning to be used for testing solely
max_test_score = max(test_scores)
test_scores_ind = [i for i, v in enumerate(test_scores) if v == max_test_score]
print('Max test score {} % and k = {}'.format(max_test_score*100,list(map(lambda x: x+1, test_scores_ind))))

# viz
K=21
centroids = df.sample(n=K)
plt.scatter(df['UMAMP_X'],df['UMAP_Y'])
plt.scatter(centroids['UMAMP_X'],centroids['UMAP_Y'],c='yellow')
plt.xlabel('UMAMP_X')
plt.ylabel('UMAP_Y')
plt.show()

#Setup a knn classifier with k neighbors
knn = KNeighborsClassifier(11)


knn.score(X_test,y_test)

from mlxtend.plotting import plot_decision_regions

#import confusion_matrix
from sklearn.metrics import confusion_matrix
#let us get the predictions using the classifier we had fit above
y_pred = knn.predict(X_test)
confusion_matrix(y_test,y_pred)
pd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)

y_pred = knn.predict(X_test)
from sklearn import metrics
cnf_matrix = metrics.confusion_matrix(y_test, y_pred)
p = sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap="YlGnBu" ,fmt='g')
plt.title('Confusion matrix', y=1.1)
plt.ylabel('Actual label')
plt.xlabel('Predicted label')

#import classification_report
from sklearn.metrics import classification_report
print(classification_report(y_test,y_pred))

# Roc curve
y_pred_proba = knn.predict_proba(X_test)[:,1]
fprate, tprate, thresholds = roc_curve(y_test, y_pred_proba, pos_label='your_label')

# Optimization 
#In case of classifier like knn the parameter to be tuned is n_neighbors
param_grid = {'n_neighbors':np.arange(1,50)}
knn = KNeighborsClassifier()
knn_cv= GridSearchCV(knn,param_grid,cv=5)
knn_cv.fit(X,y)

print("Best Score:" + str(knn_cv.best_score_))
print("Best Parameters: " + str(knn_cv.best_params_))

# Train the KNN model
from sklearn import neighbors
n_neighbors = 15
knn = neighbors.KNeighborsClassifier(n_neighbors,weights='distance')
knn_cv.fit(X,y)
# Produce the SHAP values

knn_cv.fit(X,y)

knn.fit(X_train,y_train)

pip install pygwalker

import pandas as pd
import pygwalker as pyg

import seaborn as sns
flight = sns.load_dataset('flights')
x = pyg.walk(flight)